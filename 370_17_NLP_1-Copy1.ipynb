{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 370: Natural Language Processing (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES\n",
    "- uses: translation, topic modeling, sentiment analysis, chatbots, customer service, advertising, marketing, classifying documents\n",
    "- mix of language, ML, AI\n",
    "- NLP = computational linguistics\n",
    "- natural language is often ambiguous, can be hard for computers to understand what's on\n",
    "- text data comes in large volumes\n",
    "- text is highly unstructured\n",
    "\n",
    "- NLTK and spaCy are python NLP packages\n",
    "\n",
    "- tokenization: split the longer sentences into smaller parts called tokens (sentences --> words)\n",
    "    - split string by space is an example, but has a lot of limitations, just use built in stuff\n",
    "    \n",
    "- stop words are words that appear a lot but aren't super important (think \"and\"), we can remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Please provide an example of an NLP task that you would want to perform on any text dataset you are interested in. (2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would love to do a sentiment analysis of Trump's tweets, and look for the keywords that are often negative or positively viewed. I just think this would be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first open our first NLP dataset, a Twitter sentiment dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link='https://raw.githubusercontent.com/vineetdhanawat/twitter-sentiment-analysis/master/datasets/Sentiment%20Analysis%20Dataset.csv'\n",
    "# df = pd.read_csv(link,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(link,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic text preprocessing\n",
    "\n",
    "#### Prerequisites\n",
    "From NLTK, we have already installed the following corpora using nltk.download():\n",
    "\n",
    "1. stopwords\n",
    "2. punkt\n",
    "3. wordnet\n",
    "\n",
    "These are required for doing text preprocessing using the nltk package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sentences\n",
    "\n",
    "Probably the first step, where you split longer sentences into smaller parts called tokens. Passages are tokenized to sentences, sentences to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenization function from nltk\n",
    "from nltk import word_tokenize\n",
    "sent = 'This is a sentence, awaiting to be tokenized...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence, awaiting to be tokenized...\n",
      "['This', 'is', 'a', 'sentence', ',', 'awaiting', 'to', 'be', 'tokenized', '...']\n",
      "This is a sentence , awaiting to be tokenized ...\n"
     ]
    }
   ],
   "source": [
    "# split the string into a list of tokens using the word_tokenize function\n",
    "tokens = word_tokenize(sent)\n",
    "print(sent)\n",
    "print(tokens)\n",
    "\n",
    "# re-join the tokens into a single string\n",
    "sent = ' '.join(tokens)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "Words that has small contribution to the meaning of phrases but appear frequently. It is advised to remove them in most tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# load list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  is so sad for my apl friend.............\n",
      "Stopword removed:  sad apl friend ... ... ... ... .\n",
      "Original sentence:  i missed the new moon trailer...\n",
      "Stopword removed:  missed new moon trailer ...\n",
      "Original sentence:  omg its already 7:30 :o\n",
      "Stopword removed:  omg already 7:30 :\n"
     ]
    }
   ],
   "source": [
    "# change stopwords list to set\n",
    "S = set(stopwords.words('english'))\n",
    "for sent in df['SentimentText'][:3].values:\n",
    "    sent = sent.strip().lower()\n",
    "    print(\"Original sentence: \",sent)\n",
    "    tokens = word_tokenize(sent)\n",
    "    tokens_stop_removed = []\n",
    "    for token in tokens:\n",
    "        if not token in S:\n",
    "            tokens_stop_removed.append(token)\n",
    "    sent_stop_removed = ' '.join(tokens_stop_removed)\n",
    "    print(\"Stopword removed: \",sent_stop_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some of the stopwords like 'I' still in the removed version? This is because the stopwords didn't take into account capitalized words. Therefore, lower-casing is also an important preprocessing step in several tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatizing\n",
    "\n",
    "Stemming is the process of eliminating prefixes and affixes to bring the word to its root form. Lemmatization is related to stemming, but captures canonical form of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game -> game\n",
      "gaming -> game\n",
      "gamed -> game\n",
      "games -> game\n",
      "runs -> run\n",
      "ran -> ran\n",
      "running -> run\n"
     ]
    }
   ],
   "source": [
    "words = [\"game\",\"gaming\",\"gamed\",\"games\",\"runs\",\"ran\",'running']\n",
    "for word in words:\n",
    "    print(word,'->',ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs -> dog\n",
      "corpora -> corpus\n",
      "ran -> ran\n",
      "games -> game\n",
      "dice -> dice\n"
     ]
    }
   ],
   "source": [
    "words = [\"dogs\",\"corpora\",\"ran\",\"games\",\"dice\"]\n",
    "for word in words:\n",
    "    print(word,'->',lemma.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Create a new column \"PreprocessedText\" on the dataframe \"sample\", where the column contains the text that has been (1) lower-cased, (2) tokenized, (3) stopword-removed, (4) lemmatized, (5) stemmed, and (6) re-joined into a string (4pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>PreprocessedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>sad apl friend ... ... ... ... .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>miss new moon trailer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg alreadi 7:30 :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>.. omgaga . im sooo im gunna cri . 've dentist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>think mi bf cheat ! ! ! t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText  \\\n",
       "0       1          0                       is so sad for my APL frie...   \n",
       "1       2          0                     I missed the New Moon trail...   \n",
       "2       3          1                            omg its already 7:30 :O   \n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4       5          0           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                    PreprocessedText  \n",
       "0                   sad apl friend ... ... ... ... .  \n",
       "1                          miss new moon trailer ...  \n",
       "2                                 omg alreadi 7:30 :  \n",
       "3  .. omgaga . im sooo im gunna cri . 've dentist...  \n",
       "4                        think mi bf cheat ! ! ! t_t  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df[:100]\n",
    "# type your code here\n",
    "\n",
    "preprocessed_text = []\n",
    "\n",
    "for line in sample['SentimentText'].values:\n",
    "    line = line.strip().lower()\n",
    "    tokens = word_tokenize(line)\n",
    "    tokens_out = []\n",
    "    for token in tokens:\n",
    "        if not token in S:\n",
    "            token = lemma.lemmatize(token)\n",
    "            token = ps.stem(token)\n",
    "            tokens_out.append(token)\n",
    "    line_out = ' '.join(tokens_out)\n",
    "    preprocessed_text.append(line_out)\n",
    "\n",
    "sample['PreprocessedText'] = preprocessed_text\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying text to machine learning\n",
    "\n",
    "Now we will try sentiment analysis based on the text data that we have.\n",
    "To do so, we have to structuralize the form of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slide for explanation on BOW, change to vector of vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 555)\n"
     ]
    }
   ],
   "source": [
    "# transform sentences into array\n",
    "all_sentences = sample['SentimentText']\n",
    "vectorizer.fit(all_sentences)\n",
    "X = vectorizer.transform(all_sentences)\n",
    "arr = X.toarray()\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how each word has been assigned to an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 243\n",
      "so 419\n",
      "sad 388\n",
      "for 155\n",
      "my 315\n",
      "apl 33\n",
      "friend 159\n",
      "missed 300\n",
      "the 459\n",
      "new 323\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "# print first 10 instances of vocabulary\n",
    "for i,(k,v) in enumerate(vocab.items()):\n",
    "    if i<10:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "# we can set a limitation to the number of features\n",
    "vectorizer = CountVectorizer(max_features=100)\n",
    "X = vectorizer.fit_transform(all_sentences)\n",
    "arr = X.toarray()\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "95    0\n",
      "96    0\n",
      "97    0\n",
      "98    0\n",
      "99    0\n",
      "Name: Sentiment, Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# target values y\n",
    "y = sample['Sentiment']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can transform each text sentence into vectors, we can try classification using previously learned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "# test model and get accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try printing the feature importance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 0.08926803102347022 what\n",
      "72 0.05833382740807037 see\n",
      "86 0.053160761590403775 tonight\n",
      "52 0.038130296917699 my\n",
      "83 0.036227231083201444 to\n",
      "39 0.034536118258779606 its\n",
      "28 0.03225013031154172 has\n",
      "61 0.032094465588886616 on\n",
      "79 0.02998369725702587 the\n",
      "26 0.02594745949722944 gonna\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Check feature importance\n",
    "feat_importance = rf.feature_importances_\n",
    "\n",
    "idx2word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "# get the feature importances for the top-5 features\n",
    "n=10\n",
    "for idx in feat_importance.argsort()[::-1][:n]:\n",
    "    print(idx,feat_importance[idx],idx2word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving from BOW: n-grams\n",
    "\n",
    "Instead of a single word, n-grams allows for capturing more precise phrases and expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2276)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3))  # lump together words into common phrases, acccept 1 up to 3 words\n",
    "X = vectorizer.fit_transform(all_sentences)\n",
    "arr = X.toarray()\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 885\n",
      "so 1682\n",
      "sad 1568\n",
      "for 523\n",
      "my 1208\n",
      "apl 118\n",
      "friend 541\n",
      "is so 906\n",
      "so sad 1691\n",
      "sad for 1573\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "# print first 10 instances of vocabulary\n",
    "for i,(k,v) in enumerate(vocab.items()):\n",
    "    if i<10:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Demonstrate how to change the hyperparameters from the NLP side of the model. (2pts)\n",
    "\n",
    "Changeable hyperparameters\n",
    "- level of n-grams to use\n",
    "- vocabulary size\n",
    "- size of data samples (WARNING: total data is 1M lines, so please do not use the whole dataset or it may crash the servers. ~10000 lines are acceptable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3682)\n",
      "0.85\n",
      "is 1407\n",
      "so 2699\n",
      "sad 2522\n",
      "for 810\n",
      "my 1928\n",
      "apl 188\n",
      "friend 841\n",
      "is so 1445\n",
      "so sad 2716\n",
      "sad for 2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# please write code here\n",
    "\n",
    "# increase the amount of n grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,5))\n",
    "X = vectorizer.fit_transform(all_sentences)\n",
    "arr = X.toarray()\n",
    "print(arr.shape)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(acc)\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "# vocab\n",
    "# print first 10 instances of vocabulary\n",
    "for i,(k,v) in enumerate(vocab.items()):\n",
    "    if i<10:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Try testing the model with sample sentences that you put in. What do you think the output result means? Can you see if the model seems to perform well? Under what situations do you think the model's performance is good or bad? (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1441)\n",
      "0.7333333333333333\n",
      "[[0.9 0.1]]\n",
      "[[0.9 0.1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(all_sentences)\n",
    "arr = X.toarray()\n",
    "print(arr.shape)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "acc = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "print(acc)\n",
    "\n",
    "pos_test_sentence = 'I love everything happy and good and fun!'\n",
    "pos_vec = vectorizer.transform([pos_test_sentence.lower()])\n",
    "pos_result = rf.predict_proba(pos_vec)\n",
    "print(pos_result)\n",
    "\n",
    "neg_test_sentence = 'I hate everything and it sucks and it is bad'\n",
    "neg_vec = vectorizer.transform([neg_test_sentence.lower()])\n",
    "neg_result = rf.predict_proba(neg_vec)\n",
    "print(neg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote one pretty negative and one pretty positive one, and it said 90% they were both negative, so it's not a great model for me at least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the left number is the probability of having the class 0, which means negative\n",
    "the right number is the probability of having the class 1, which means positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
